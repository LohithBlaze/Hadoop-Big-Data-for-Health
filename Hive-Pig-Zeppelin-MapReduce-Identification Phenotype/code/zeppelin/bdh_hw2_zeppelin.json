{"paragraphs":[{"text":"%md ##Event Statistics using Scala","dateUpdated":"2017-02-12T18:25:09-0500","config":{"editorMode":"ace/mode/markdown","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"markdown","editOnDblClick":true}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h2>Event Statistics using Scala</h2>\n"}]},"apps":[],"jobName":"paragraph_1486941909994_-1424658820","id":"20170126-084346_411967885","dateCreated":"2017-02-12T18:25:09-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:215"},{"title":"Event Statistics using Scala","text":"// any import statements go here\nimport org.apache.spark.rdd.RDD\nimport org.apache.commons.io.IOUtils\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.net.URL\nimport java.nio.charset.Charset\nimport org.apache.spark.sql._","dateUpdated":"2017-02-12T19:15:14-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.commons.io.IOUtils\n\nimport java.text.SimpleDateFormat\n\nimport java.util.Date\n\nimport java.net.URL\n\nimport java.nio.charset.Charset\n\nimport org.apache.spark.sql._\n"}]},"apps":[],"jobName":"paragraph_1486941909998_-1426197816","id":"20170125-092928_572765858","dateCreated":"2017-02-12T18:25:09-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:216","user":"anonymous","dateFinished":"2017-02-12T19:15:15-0500","dateStarted":"2017-02-12T19:15:14-0500"},{"title":"Then, load data. We uploaded the data on S3 for easier loading","text":"// load raw data\n\ndef loadFromUrl(url:String) = \n    sc.parallelize(\n        IOUtils.toString(\n            new URL(url),\n            Charset.forName(\"utf8\")).split(\"\\n\"))\n            \nval events = loadFromUrl(\"https://s3.amazonaws.com/6250bdh-hw/hw2/train/events.csv\")\nval mortality = loadFromUrl(\"https://s3.amazonaws.com/6250bdh-hw/hw2/train/mortality.csv\")\n","dateUpdated":"2017-02-12T19:15:16-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nloadFromUrl: (url: String)org.apache.spark.rdd.RDD[String]\n\nevents: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[128] at parallelize at <console>:81\n\nmortality: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[129] at parallelize at <console>:81\n"}]},"apps":[],"jobName":"paragraph_1486941909999_-1426582565","id":"20170125-102007_1054430570","dateCreated":"2017-02-12T18:25:09-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:217","user":"anonymous","dateFinished":"2017-02-12T19:15:31-0500","dateStarted":"2017-02-12T19:15:16-0500"},{"text":"import java.util.Date\n// Define case class\ncase class Event(patientId: String, category: String, event: String, date: java.util.Date, value: Double)\ncase class Mortality(patientId: String, mortality_date:  java.util.Date, label: Double)\n","dateUpdated":"2017-02-12T19:15:33-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport java.util.Date\n\ndefined class Event\n\ndefined class Mortality\n"}]},"apps":[],"jobName":"paragraph_1486941910000_-1416194344","id":"20170125-093656_1081259042","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:218","user":"anonymous","dateFinished":"2017-02-12T19:15:34-0500","dateStarted":"2017-02-12T19:15:33-0500"},{"text":"\n// Define date format\nval dateFormat = new SimpleDateFormat(\"yyyy-MM-dd\")","dateUpdated":"2017-02-12T19:15:35-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndateFormat: java.text.SimpleDateFormat = java.text.SimpleDateFormat@f67a0200\n"}]},"apps":[],"jobName":"paragraph_1486941910001_-1416579093","id":"20170205-182828_2037956217","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:219","user":"anonymous","dateFinished":"2017-02-12T19:15:35-0500","dateStarted":"2017-02-12T19:15:35-0500"},{"text":"\n// Load events & mortality into their corresponding RDD\ndef myStringToDouble(s: String): Option[Double] = {\n try {\n   Some(s.toDouble)\n } catch {\n   case e: Exception => None\n }\n}\n//val eventsRDD: RDD[Event] = events.map(s=>s.split(\",\")).map(s=>Event(s(0), s(1), s(2),dateFormat.parse(s(3).asInstanceOf[String]), s(4).toDouble))\n//val mortalityRDD: RDD[Mortality] = mortality.map(s=>s.split(\",\")).map(s=>Mortality(s(0), dateFormat.parse(s(1).asInstanceOf[String]), s(2).toDouble))\nval eventsRDD: RDD[Event] = events.map(s=>s.split(\",\")).map(s=>Event(s(0), s(1), s(2),dateFormat.parse(s(3).asInstanceOf[String]),myStringToDouble(s(4)).getOrElse(0.0)))\nval mortalityRDD: RDD[Mortality] = mortality.map(s=>s.split(\",\")).map(s=>Mortality(s(0), dateFormat.parse(s(1).asInstanceOf[String]),myStringToDouble(s(2)).getOrElse(0.0)))\n\n\n//val keyedEventsRDD: RDD[(String, Event)] = eventsRDD.map(s=>(s.patientId,s))\n//val keyedMortalityRDD: RDD[(String, Mortality)] = mortalityRDD.map(s=>(s.patientId,s))\n//val joinedRDD: RDD[(String,(Event,Option[Mortality]))] = keyedEventsRDD.leftOuterJoin(keyedMortalityRDD)\n//val deadRDD: RDD[(String,(Event,Option[Mortality]))] = joinedRDD.filter(s=> s._2._2.isDefined)\n//val aliveRDD: RDD[(String,(Event,Option[Mortality]))] = joinedRDD.filter(s=> !s._2._2.isDefined)    ","dateUpdated":"2017-02-12T19:15:44-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nmyStringToDouble: (s: String)Option[Double]\n\neventsRDD: org.apache.spark.rdd.RDD[Event] = MapPartitionsRDD[131] at map at <console>:89\n\nmortalityRDD: org.apache.spark.rdd.RDD[Mortality] = MapPartitionsRDD[133] at map at <console>:89\n"}]},"apps":[],"jobName":"paragraph_1486941910002_-1415424846","id":"20170125-103206_1230836042","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:220","user":"anonymous","dateFinished":"2017-02-12T19:15:39-0500","dateStarted":"2017-02-12T19:15:38-0500"},{"text":"//aliveRDD.take(5).foreach(println)\n//println(joinedRDD.count) \n//println(eventsRDD.count) 3073588\n//println(deadRDD.count) 1029059\n//println(aliveRDD.count) 2044529\n//val deadByKeyforTesting = deadRDD.take(10)\n//val dead=deadByKeyforTesting.map{x=> (x._1,x._2._1)}\n//val groupedDead=dead.groupBy(x=>x._1).mapValues(_.size)\n//groupedDead.foreach(println)","dateUpdated":"2017-02-12T19:15:46-0500","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"},"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1486941910002_-1415424846","id":"20170212-161409_93099056","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:221","user":"anonymous","dateFinished":"2017-02-12T19:15:46-0500","dateStarted":"2017-02-12T19:15:46-0500"},{"title":"Event count is defined as the number of events recorded for a given patient","text":"def event_count_metrics(eve: RDD[(Event)], mor: RDD[(Mortality)]) : (Double, Double, Double, Double, Double, Double) = {\n    \n    // TODO : Implement this function to return the event count metrics.\n    val keyedEventsRDD: RDD[(String, Event)] = eventsRDD.map(s=>(s.patientId,s))  \n    val keyedMortalityRDD: RDD[(String, Mortality)] = mortalityRDD.map(s=>(s.patientId,s))\n    val joinedRDD: RDD[(String,(Event,Option[Mortality]))] = keyedEventsRDD.leftOuterJoin(keyedMortalityRDD)\n    val deadRDD: RDD[(String,(Event,Option[Mortality]))] = joinedRDD.filter(s=> s._2._2.isDefined)\n    val aliveRDD: RDD[(String,(Event,Option[Mortality]))] = joinedRDD.filter(s=> !s._2._2.isDefined)\n    \n    val deadcountList = deadRDD.map{x=>(x._1,x._2._1)}.groupBy(x=>x._1).mapValues(_.size)\n    val alivecountList = aliveRDD.map{x=>(x._1,x._2._1)}.groupBy(x=>x._1).mapValues(_.size)\n    val deadcount=deadcountList.map(x=>x._2.toDouble)\n    val alivecount=alivecountList.map(x=>x._2.toDouble)\n    val avg_dead_event_count = deadcount.mean()\n    val max_dead_event_count = deadcount.max()\n    val min_dead_event_count = deadcount.min()\n    val avg_alive_event_count = alivecount.mean()\n    val max_alive_event_count = alivecount.max()\n    val min_alive_event_count = alivecount.min()\n    \n    (avg_dead_event_count, max_dead_event_count, min_dead_event_count, avg_alive_event_count, max_alive_event_count, min_alive_event_count)\n}\n","dateUpdated":"2017-02-12T19:15:49-0500","config":{"tableHide":false,"editorMode":"ace/mode/scala","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nevent_count_metrics: (eve: org.apache.spark.rdd.RDD[Event], mor: org.apache.spark.rdd.RDD[Mortality])(Double, Double, Double, Double, Double, Double)\n"}]},"apps":[],"jobName":"paragraph_1486941910003_-1415809595","id":"20170125-163824_794924019","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:222","user":"anonymous","dateFinished":"2017-02-12T19:15:49-0500","dateStarted":"2017-02-12T19:15:49-0500"},{"title":"Encounter count is defined as the count of unique dates on which a given patient visited the ICU. ","text":"def encounter_count_metrics(eve: RDD[(Event)], mor: RDD[(Mortality)]) : (Double, Double, Double, Double, Double, Double) = {\n    \n    // TODO : Implement this function to return the encounter count metrics.\n    val keyedEventsRDD: RDD[(String, Event)] = eventsRDD.map(s=>(s.patientId,s))  \n    val keyedMortalityRDD: RDD[(String, Mortality)] = mortalityRDD.map(s=>(s.patientId,s))\n    val joinedRDD: RDD[(String,(Event,Option[Mortality]))] = keyedEventsRDD.leftOuterJoin(keyedMortalityRDD)\n    val deadRDD: RDD[(String,(Event,Option[Mortality]))] = joinedRDD.filter(s=> s._2._2.isDefined)\n    val aliveRDD: RDD[(String,(Event,Option[Mortality]))] = joinedRDD.filter(s=> !s._2._2.isDefined)\n    \n    val deadencounterList = deadRDD.map(x=>{val event=x._2._1; val patientid=event.patientId; val time=event.date; (patientid,time)}).distinct.groupBy(x=>x._1).mapValues(_.size)\n    val aliveencounterList = aliveRDD.map(x=>{val event=x._2._1; val patientid=event.patientId; val time=event.date; (patientid,time)}).distinct.groupBy(x=>x._1).mapValues(_.size)\n    val deadencounter=deadencounterList.map(x=>x._2.toDouble)\n    val aliveencounter=aliveencounterList.map(x=>x._2.toDouble)\n    \n    val avg_dead_encounter_count = deadencounter.mean()\n    val max_dead_encounter_count = deadencounter.max()\n    val min_dead_encounter_count = deadencounter.min()\n    val avg_alive_encounter_count = aliveencounter.mean()\n    val max_alive_encounter_count = aliveencounter.max()\n    val min_alive_encounter_count = aliveencounter.min()\n    \n    (avg_dead_encounter_count, max_dead_encounter_count, min_dead_encounter_count, avg_alive_encounter_count, max_alive_encounter_count, min_alive_encounter_count)\n}","dateUpdated":"2017-02-12T19:15:52-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1486941910004_-1417733340","id":"20170126-094037_1369751422","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:223","user":"anonymous","dateFinished":"2017-02-12T19:15:53-0500","dateStarted":"2017-02-12T19:15:52-0500","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nencounter_count_metrics: (eve: org.apache.spark.rdd.RDD[Event], mor: org.apache.spark.rdd.RDD[Mortality])(Double, Double, Double, Double, Double, Double)\n"}]}},{"title":"Testing Event Count - Don't change this cell","text":"\nval (min_dead_event_count, max_dead_event_count, avg_dead_event_count, min_alive_event_count, max_alive_event_count, avg_alive_event_count) = \nevent_count_metrics(eventsRDD, mortalityRDD)","dateUpdated":"2017-02-12T19:15:55-0500","config":{"tableHide":false,"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1486941910005_-1418118089","id":"20170125-164106_1373358169","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:224","user":"anonymous","dateFinished":"2017-02-12T19:17:36-0500","dateStarted":"2017-02-12T19:15:55-0500","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n\n\n\n\n\nmin_dead_event_count: Double = 1029.059\nmax_dead_event_count: Double = 16829.0\navg_dead_event_count: Double = 2.0\nmin_alive_event_count: Double = 682.6474123539235\nmax_alive_event_count: Double = 12627.0\navg_alive_event_count: Double = 1.0\n"}]}},{"title":"Populate the correct values in df_events dataframe","text":"case class eventRecord(Average_Event: Double , Max_Event: Double, Min_Event: Double, Mortality: String) \n//val df_events = Seq(eventRecord(50.0, 80.0, 20.0, \"Alive\"), eventRecord(100.0, 160.0, 60.0, \"Dead\")).toDF \n\n// TODO - Fill in the correct values of minimum, maximum and average events for Alive and Dead Patients \nval df_events = Seq(eventRecord(avg_alive_event_count, max_alive_event_count, min_alive_event_count, \"Alive\"), eventRecord(avg_dead_event_count, max_dead_event_count, min_dead_event_count, \"Dead\")).toDF\ndf_events.registerTempTable(\"df_events\")\n","dateUpdated":"2017-02-12T19:20:25-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1486941910005_-1418118089","id":"20170126-095056_275615884","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:225","user":"anonymous","dateFinished":"2017-02-12T19:20:26-0500","dateStarted":"2017-02-12T19:20:25-0500","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndefined class eventRecord\n\ndf_events: org.apache.spark.sql.DataFrame = [Average_Event: double, Max_Event: double ... 2 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"}]}},{"title":"Plot Event Count Grouped by Dead/Alive","text":"%sql\nselect * from df_events","dateUpdated":"2017-02-12T19:21:06-0500","config":{"editorMode":"ace/mode/sql","colWidth":12,"title":true,"results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"keys":[{"name":"Average_Event","index":0,"aggr":"sum"},{"name":"Max_Event","index":1,"aggr":"sum"},{"name":"Min_Event","index":2,"aggr":"sum"}],"values":[{"name":"Average_Event","index":0,"aggr":"sum"},{"name":"Max_Event","index":1,"aggr":"sum"},{"name":"Min_Event","index":2,"aggr":"sum"}],"groups":[{"name":"Mortality","index":3,"aggr":"sum"}],"scatter":{"xAxis":{"name":"Average_Event","index":0,"aggr":"sum"},"yAxis":{"name":"Max_Event","index":1,"aggr":"sum"}},"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{}},"helium":{}}],"enabled":true,"editorSetting":{"language":"sql"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Average_Event\tMax_Event\tMin_Event\tMortality\n1.0\t12627.0\t682.6474123539235\tAlive\n2.0\t16829.0\t1029.059\tDead\n"}]},"apps":[],"jobName":"paragraph_1486941910006_-1416963842","id":"20170127-103258_1100387642","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:226","user":"anonymous","dateFinished":"2017-02-12T19:20:39-0500","dateStarted":"2017-02-12T19:20:39-0500"},{"title":"Testing Encounter Count - Don't change any cell starting from this one","text":"val  (avg_dead_encounter_count, max_dead_encounter_count, min_dead_encounter_count, avg_alive_encounter_count, max_alive_encounter_count, min_alive_encounter_count) = encounter_count_metrics(eventsRDD, mortalityRDD)","dateUpdated":"2017-02-12T19:21:13-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1486941910006_-1416963842","id":"20170126-085842_586212247","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:227","user":"anonymous","dateFinished":"2017-02-12T19:21:40-0500","dateStarted":"2017-02-12T19:21:13-0500","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n\n\n\n\n\navg_dead_encounter_count: Double = 24.861\nmax_dead_encounter_count: Double = 375.0\nmin_dead_encounter_count: Double = 1.0\navg_alive_encounter_count: Double = 18.669449081803027\nmax_alive_encounter_count: Double = 391.0\nmin_alive_encounter_count: Double = 1.0\n"}]}},{"title":"Populate the correct values in df_encounters dataframe","text":"case class encounterRecord(Average_Encounter: Double , Max_Encounter: Double, Min_Encounter: Double, Mortality: String)\n\n// val df_encounter = Seq(encounterRecord(50.0, 80.0, 20.0, \"Alive\"), encounterRecord(100.0, 160.0, 60.0, \"Dead\")).toDF \n\n// TODO - Fill in the correct values of minimum, maximum and average events for Alive and Dead Patients \nval df_encounter = Seq(encounterRecord(avg_alive_encounter_count, max_alive_encounter_count, min_alive_encounter_count, \"Alive\"), encounterRecord(avg_dead_encounter_count, max_dead_encounter_count, min_alive_encounter_count, \"Dead\")).toDF\ndf_encounter.registerTempTable(\"df_encounter\")\n","dateUpdated":"2017-02-12T19:22:18-0500","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1486941910007_-1417348591","id":"20170127-104258_320884595","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:228","user":"anonymous","dateFinished":"2017-02-12T19:22:19-0500","dateStarted":"2017-02-12T19:22:18-0500","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndefined class encounterRecord\n\ndf_encounter: org.apache.spark.sql.DataFrame = [Average_Encounter: double, Max_Encounter: double ... 2 more fields]\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"}]}},{"title":"Plot Encounter Count Grouped by Dead/Alive","text":"%sql\nselect * from df_encounter ","dateUpdated":"2017-02-12T19:22:20-0500","config":{"colWidth":12,"title":true,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":true,"keys":[{"name":"Max_Encounter","index":1,"aggr":"sum"},{"name":"Average_Encounter","index":0,"aggr":"sum"},{"name":"Min_Encounter","index":2,"aggr":"sum"}],"values":[{"name":"Max_Encounter","index":1,"aggr":"sum"},{"name":"Average_Encounter","index":0,"aggr":"sum"},{"name":"Min_Encounter","index":2,"aggr":"sum"}],"groups":[{"name":"Mortality","index":3,"aggr":"sum"}],"scatter":{"xAxis":{"name":"Average_Encounter","index":0,"aggr":"sum"},"yAxis":{"name":"Max_Encounter","index":1,"aggr":"sum"}}},"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":true,"setting":{"multiBarChart":{}},"commonSetting":{},"keys":[{"name":"Average_Encounter","index":0,"aggr":"sum"},{"name":"Max_Encounter","index":1,"aggr":"sum"},{"name":"Min_Encounter","index":2,"aggr":"sum"}],"groups":[{"name":"Mortality","index":3,"aggr":"sum"}],"values":[{"name":"Average_Encounter","index":0,"aggr":"sum"},{"name":"Max_Encounter","index":1,"aggr":"sum"},{"name":"Min_Encounter","index":2,"aggr":"sum"}]},"helium":{}}},"editorSetting":{"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1486941910007_-1417348591","id":"20170127-113147_1780103981","dateCreated":"2017-02-12T18:25:10-0500","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:229","user":"anonymous","dateFinished":"2017-02-12T19:22:20-0500","dateStarted":"2017-02-12T19:22:20-0500","results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Average_Encounter\tMax_Encounter\tMin_Encounter\tMortality\n18.669449081803027\t391.0\t1.0\tAlive\n24.861\t375.0\t1.0\tDead\n"}]}},{"text":"","dateUpdated":"2017-02-12T18:25:10-0500","config":{"colWidth":12,"enabled":true,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1486941910008_-1419272335","id":"20170127-113337_1396101507","dateCreated":"2017-02-12T18:25:10-0500","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230"}],"name":"Tian Tan","id":"2CAW1US9D","angularObjects":{"2C7UXYA55:shared_process":[],"2C8XG7KZD:shared_process":[],"2C98P26AG:shared_process":[],"2C7ZQRMMW:shared_process":[],"2C8NNEFSX:shared_process":[],"2CBJ24Z14:shared_process":[],"2CAPUCEM7:shared_process":[],"2CADR3WF3:shared_process":[],"2C7T54VWK:shared_process":[],"2C928XFDF:shared_process":[],"2C889M3EF:shared_process":[],"2CAZEB8V2:shared_process":[],"2C9PY1EES:shared_process":[],"2C8D7DADU:shared_process":[],"2C9FQE73T:shared_process":[],"2C82QP8QJ:shared_process":[],"2CAYQTUYV:shared_process":[],"2C9CGJ44G:shared_process":[],"2CB2UH2FJ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}